{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!pip -q install opencv-python pycocotools tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2019,
     "status": "ok",
     "timestamp": 1765901553861,
     "user": {
      "displayName": "Shanna Fernlie",
      "userId": "14829187662547582475"
     },
     "user_tz": -420
    },
    "id": "JpeXsfw43KVg",
    "outputId": "3a153291-952e-4b3b-89d3-35ba8d217b3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "import os, json, random, math, time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely import wkt\n",
    "import cv2, os\n",
    "from segment_anything import sam_model_registry\n",
    "from segment_anything.utils.transforms import ResizeLongestSide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9697,
     "status": "ok",
     "timestamp": 1765901400387,
     "user": {
      "displayName": "Shanna Fernlie",
      "userId": "14829187662547582475"
     },
     "user_tz": -420
    },
    "id": "rRLGz9-k4bgC",
    "outputId": "1ac4df89-7435-4935-f248-b5224ec5dcb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10109,
     "status": "ok",
     "timestamp": 1765901110655,
     "user": {
      "displayName": "Shanna Fernlie",
      "userId": "14829187662547582475"
     },
     "user_tz": -420
    },
    "id": "wyd3HhoOIwRA",
    "outputId": "20e0dd66-70c5-4f90-c86b-e4fa23583ee5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
      "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-svolwcv4\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-svolwcv4\n",
      "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: segment_anything\n",
      "  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for segment_anything: filename=segment_anything-1.0-py3-none-any.whl size=36592 sha256=4b2d195e112f595baae2e9892d5f3cf2affbfd67b33a9cf12ad0853d641adf8d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-et2lai1n/wheels/29/82/ff/04e2be9805a1cb48bec0b85b5a6da6b63f647645750a0e42d4\n",
      "Successfully built segment_anything\n",
      "Installing collected packages: segment_anything\n",
      "  Attempting uninstall: segment_anything\n",
      "    Found existing installation: segment_anything 1.0\n",
      "    Uninstalling segment_anything-1.0:\n",
      "      Successfully uninstalled segment_anything-1.0\n",
      "Successfully installed segment_anything-1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/facebookresearch/segment-anything.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7774,
     "status": "ok",
     "timestamp": 1765901062268,
     "user": {
      "displayName": "Shanna Fernlie",
      "userId": "14829187662547582475"
     },
     "user_tz": -420
    },
    "id": "_2AiFeR3E_r9",
    "outputId": "4cab599c-3656-4d00-9d4e-096c971c187d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!rm -rf /content/segment-anything\n",
    "!git clone -q https://github.com/facebookresearch/segment-anything.git /content/segment-anything\n",
    "!pip -q install -e /content/segment-anything\n",
    "!pip -q install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2976,
     "status": "ok",
     "timestamp": 1765901541213,
     "user": {
      "displayName": "Shanna Fernlie",
      "userId": "14829187662547582475"
     },
     "user_tz": -420
    },
    "id": "rIo5fRdAFBlF",
    "outputId": "23f42bfa-8965-4f5b-823a-f8519e33f16b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n",
      "OUT_DIR: /content/drive/MyDrive/Kuliah/Skripsi S1/checkpoints\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "IMAGE_DIR = \"/content/drive/MyDrive/Kuliah/Skripsi S1/train/images\"\n",
    "LABEL_DIR = \"/content/drive/MyDrive/Kuliah/Skripsi S1/train/labels\"\n",
    "\n",
    "SAM_CKPT  = \"/content/drive/MyDrive/Kuliah/Skripsi S1/checkpoints/sam_vit_b_01ec64.pth\"\n",
    "MODEL_TYPE = \"vit_b\"\n",
    "\n",
    "OUT_DIR = \"/content/drive/MyDrive/Kuliah/Skripsi S1/checkpoints\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 2\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 0.0\n",
    "GRAD_ACCUM_STEPS = 8\n",
    "AMP = True\n",
    "INSTANCES_PER_IMAGE = 32\n",
    "\n",
    "\n",
    "VAL_RATIO = 0.1\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1btt5BI44dhm"
   },
   "outputs": [],
   "source": [
    "def list_images(image_dir):\n",
    "    exts = (\".png\")\n",
    "    files = [f for f in os.listdir(image_dir) if f.lower().endswith(exts)]\n",
    "    files.sort()\n",
    "    return files\n",
    "\n",
    "def compute_bbox_from_mask(bin_mask):\n",
    "    ys, xs = np.where(bin_mask > 0)\n",
    "    if len(xs) == 0 or len(ys) == 0:\n",
    "        return None\n",
    "    x0, x1 = xs.min(), xs.max()\n",
    "    y0, y1 = ys.min(), ys.max()\n",
    "    return np.array([x0, y0, x1, y1], dtype=np.float32)\n",
    "\n",
    "def polygon_to_mask(poly, h, w):\n",
    "    mask = np.zeros((h, w), dtype=np.uint8)\n",
    "\n",
    "    ext = np.array(list(poly.exterior.coords), dtype=np.float32)\n",
    "    ext[:, 0] = np.clip(ext[:, 0], 0, w - 1)\n",
    "    ext[:, 1] = np.clip(ext[:, 1], 0, h - 1)\n",
    "    ext_i = np.round(ext).astype(np.int32)\n",
    "    cv2.fillPoly(mask, [ext_i], 1)\n",
    "\n",
    "    for ring in poly.interiors:\n",
    "        hole = np.array(list(ring.coords), dtype=np.float32)\n",
    "        hole[:, 0] = np.clip(hole[:, 0], 0, w - 1)\n",
    "        hole[:, 1] = np.clip(hole[:, 1], 0, h - 1)\n",
    "        hole_i = np.round(hole).astype(np.int32)\n",
    "        cv2.fillPoly(mask, [hole_i], 0)\n",
    "\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "error",
     "timestamp": 1765901547111,
     "user": {
      "displayName": "Shanna Fernlie",
      "userId": "14829187662547582475"
     },
     "user_tz": -420
    },
    "id": "a5UUhlLg4fug",
    "outputId": "0b49433f-6507-4056-aa33-157d7c12290a"
   },
   "outputs": [],
   "source": [
    "class XBDWKTInstanceDataset(Dataset):\n",
    "    def __init__(self, files, image_dir, label_dir, instances_per_image=16):\n",
    "        self.files = files\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.instances_per_image = instances_per_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        base = os.path.splitext(fname)[0]\n",
    "\n",
    "        img_path = os.path.join(self.image_dir, fname)\n",
    "        json_path = os.path.join(self.label_dir, base + \".json\")\n",
    "\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        if not os.path.exists(json_path):\n",
    "            raise FileNotFoundError(f\"Missing JSON: {json_path}\")\n",
    "\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        feats = data.get(\"features\", {}).get(\"xy\", [])  # ✅ FIXED\n",
    "        masks = []\n",
    "\n",
    "        for feat in feats:\n",
    "            wkt_str = feat.get(\"wkt\", None)\n",
    "            if not wkt_str:\n",
    "                continue\n",
    "\n",
    "            geom = wkt.loads(wkt_str)\n",
    "\n",
    "            if geom.geom_type == \"Polygon\":\n",
    "                m = polygon_to_mask(geom, h, w)\n",
    "                if m.sum() > 0:\n",
    "                    masks.append(m)\n",
    "\n",
    "            elif geom.geom_type == \"MultiPolygon\":\n",
    "                for poly in geom.geoms:\n",
    "                    m = polygon_to_mask(poly, h, w)\n",
    "                    if m.sum() > 0:\n",
    "                        masks.append(m)\n",
    "\n",
    "        if len(masks) == 0:\n",
    "            gt = np.zeros((h, w), dtype=np.uint8)\n",
    "            bbox = np.array([0, 0, w-1, h-1], dtype=np.float32)\n",
    "            return [(img, gt, bbox)]\n",
    "\n",
    "        k = min(self.instances_per_image, len(masks))\n",
    "        chosen = random.sample(masks, k)\n",
    "\n",
    "        samples = []\n",
    "        for gt in chosen:\n",
    "            bbox = compute_bbox_from_mask(gt)\n",
    "            if bbox is None:\n",
    "                continue\n",
    "            samples.append((img, gt, bbox))\n",
    "\n",
    "        if len(samples) == 0:\n",
    "            gt = np.zeros((h, w), dtype=np.uint8)\n",
    "            bbox = np.array([0, 0, w-1, h-1], dtype=np.float32)\n",
    "            samples = [(img, gt, bbox)]\n",
    "\n",
    "        return samples\n",
    "\n",
    "def collate_flatten(batch):\n",
    "    flat = []\n",
    "    for item in batch:\n",
    "        flat.extend(item if isinstance(item, list) else [item])\n",
    "    imgs, gts, bboxes = zip(*flat)\n",
    "    return list(imgs), list(gts), torch.tensor(np.stack(bboxes), dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24680,
     "status": "ok",
     "timestamp": 1765818804810,
     "user": {
      "displayName": "Shanna Fernlie",
      "userId": "14829187662547582475"
     },
     "user_tz": -420
    },
    "id": "rWMtFJ1Y4g5I",
    "outputId": "c01195ae-a18a-43ed-a20c-25ab08c7f6e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images: 5598\n",
      "pre only: 2799\n",
      "Total images: 2799\n",
      "Train images: 2520\n",
      "Val images: 279\n"
     ]
    }
   ],
   "source": [
    "all_files = list_images(IMAGE_DIR)\n",
    "\n",
    "PRE_KEYWORD = ['pre_disaster']\n",
    "\n",
    "pre_files = []\n",
    "for f in all_files:\n",
    "  name = f.lower()\n",
    "  if any(k in name for k in PRE_KEYWORD) and (\"post\" not in name):\n",
    "        pre_files.append(f)\n",
    "\n",
    "print(\"All images:\", len(all_files))\n",
    "print(\"pre only:\", len(pre_files))\n",
    "\n",
    "random.seed(SEED)\n",
    "random.shuffle(pre_files)\n",
    "\n",
    "n_val = int(len(pre_files) * VAL_RATIO)\n",
    "val_files = pre_files[:n_val]\n",
    "train_files = pre_files[n_val:]\n",
    "\n",
    "train_ds = XBDWKTInstanceDataset(train_files, IMAGE_DIR, LABEL_DIR, instances_per_image=INSTANCES_PER_IMAGE)\n",
    "val_ds   = XBDWKTInstanceDataset(val_files,   IMAGE_DIR, LABEL_DIR, instances_per_image=INSTANCES_PER_IMAGE)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_flatten\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_flatten\n",
    ")\n",
    "\n",
    "print(\"Total images:\", len(pre_files))\n",
    "print(\"Train images:\", len(train_ds))\n",
    "print(\"Val images:\", len(val_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 567,
     "status": "ok",
     "timestamp": 1765818842403,
     "user": {
      "displayName": "Shanna Fernlie",
      "userId": "14829187662547582475"
     },
     "user_tz": -420
    },
    "id": "pLp46iCU7dys",
    "outputId": "3ac69bba-5841-4130-a577-f1c1cf577d39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON path: /content/drive/MyDrive/Kuliah/Skripsi S1/train/labels/guatemala-volcano_00000002_pre_disaster.json\n",
      "Exists: True\n",
      "Top-level keys: dict_keys(['features', 'metadata'])\n",
      "xy exists: False\n",
      "Number of features: 1\n",
      "type: <class 'list'>\n",
      "len: 1\n",
      "Feature keys: dict_keys(['properties', 'wkt'])\n",
      "WKT head: POLYGON ((1024 238.0106127841054, 1020.035908691126 233.848316909788, 1016.666431078584 237.2177945223307, 1011.909521507935 232.4608849516822, 1008.341839329949 236.2267716951123, 1001.40467953942 23\n"
     ]
    }
   ],
   "source": [
    "fname = \"guatemala-volcano_00000002_pre_disaster.png\"\n",
    "base = os.path.splitext(fname)[0]\n",
    "json_path = os.path.join(LABEL_DIR, base + \".json\")\n",
    "\n",
    "print(\"JSON path:\", json_path)\n",
    "print(\"Exists:\", os.path.exists(json_path))\n",
    "\n",
    "with open(json_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"Top-level keys:\", data.keys())\n",
    "print(\"xy exists:\", \"xy\" in data)\n",
    "print(\"Number of features:\", len(data[\"features\"][\"xy\"]))\n",
    "\n",
    "feat_list = data[\"features\"][\"xy\"]\n",
    "print(\"type:\", type(feat_list))\n",
    "print(\"len:\", len(feat_list))\n",
    "\n",
    "feat = feat_list[0]\n",
    "print(\"Feature keys:\", feat.keys())\n",
    "print(\"WKT head:\", feat[\"wkt\"][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1620,
     "status": "ok",
     "timestamp": 1765818845976,
     "user": {
      "displayName": "Shanna Fernlie",
      "userId": "14829187662547582475"
     },
     "user_tz": -420
    },
    "id": "Pu5iPNcn9M5Y",
    "outputId": "624ebcb9-8616-4ff0-e4ee-c41865d41491"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geom type: Polygon\n",
      "bounds: (983.368064084044, 230.0824301663579, 1024.0, 335.1360018036964)\n",
      "image w,h: 1024 1024\n"
     ]
    }
   ],
   "source": [
    "geom = wkt.loads(feat[\"wkt\"])\n",
    "print(\"geom type:\", geom.geom_type)\n",
    "print(\"bounds:\", geom.bounds)\n",
    "\n",
    "img = cv2.imread(os.path.join(IMAGE_DIR, fname))\n",
    "h, w = img.shape[:2]\n",
    "print(\"image w,h:\", w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15120,
     "status": "ok",
     "timestamp": 1765818861899,
     "user": {
      "displayName": "Shanna Fernlie",
      "userId": "14829187662547582475"
     },
     "user_tz": -420
    },
    "id": "r_qnDjtI4jFH",
    "outputId": "3de8fd0d-6da5-45b4-8049-36462bb0bb07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAM loaded: vit_b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2730337682.py:25: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=AMP)\n"
     ]
    }
   ],
   "source": [
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=SAM_CKPT).to(DEVICE)\n",
    "sam.train()\n",
    "\n",
    "# Freeze image encoder + prompt encoder (cheaper)\n",
    "for p in sam.image_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in sam.prompt_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Train mask decoder\n",
    "for p in sam.mask_decoder.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "transform = ResizeLongestSide(sam.image_encoder.img_size)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    sam.mask_decoder.parameters(),\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=AMP)\n",
    "\n",
    "print(\"SAM loaded:\", MODEL_TYPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AtAetMJDJiV"
   },
   "outputs": [],
   "source": [
    "bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def dice_loss(logits, targets, eps=1e-6):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    probs = probs.flatten(1)\n",
    "    targets = targets.flatten(1)\n",
    "    num = 2 * (probs * targets).sum(dim=1)\n",
    "    den = (probs + targets).sum(dim=1) + eps\n",
    "    return 1 - (num / den).mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def dice_iou_from_logits(logits, targets, thresh=0.5, eps=1e-6):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs > thresh).float()\n",
    "\n",
    "    preds = preds.flatten(1)\n",
    "    targets = targets.flatten(1)\n",
    "\n",
    "    inter = (preds * targets).sum(dim=1)\n",
    "    union = (preds + targets - preds * targets).sum(dim=1)\n",
    "\n",
    "    dice = (2 * inter + eps) / (preds.sum(dim=1) + targets.sum(dim=1) + eps)\n",
    "    iou  = (inter + eps) / (union + eps)\n",
    "    return dice.mean().item(), iou.mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0d70U7xRFhf9"
   },
   "outputs": [],
   "source": [
    "def preprocess_image_np(img_rgb):\n",
    "    resized = transform.apply_image(img_rgb)\n",
    "    t = torch.as_tensor(resized, device=DEVICE).permute(2, 0, 1).contiguous()[None, ...]\n",
    "    t = sam.preprocess(t)\n",
    "    resized_hw = resized.shape[:2]\n",
    "    return t, resized_hw\n",
    "\n",
    "def preprocess_mask_np(mask_uint8):\n",
    "    return torch.from_numpy(mask_uint8.astype(np.float32))[None, None, ...]  # 1x1xH0xW0\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_sam(sam_model, loader, max_instances=800):\n",
    "    sam_model.eval()\n",
    "    dice_list, iou_list = [], []\n",
    "    count = 0\n",
    "\n",
    "    for imgs, gts, bboxes in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        for i in range(len(imgs)):\n",
    "            img = imgs[i]\n",
    "            gt  = gts[i]\n",
    "            bbox = bboxes[i].cpu().numpy()\n",
    "\n",
    "            original_size = img.shape[:2]\n",
    "            input_image, resized_hw = preprocess_image_np(img)\n",
    "\n",
    "            image_embedding = sam_model.image_encoder(input_image)\n",
    "\n",
    "            box = transform.apply_boxes(bbox[None, :].astype(np.float32), original_size)\n",
    "            box_torch = torch.as_tensor(box, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "            sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
    "                points=None, boxes=box_torch, masks=None\n",
    "            )\n",
    "\n",
    "            low_res_masks, _ = sam_model.mask_decoder(\n",
    "                image_embeddings=image_embedding,\n",
    "                image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "                sparse_prompt_embeddings=sparse_embeddings,\n",
    "                dense_prompt_embeddings=dense_embeddings,\n",
    "                multimask_output=False,\n",
    "            )\n",
    "\n",
    "            up_masks = sam_model.postprocess_masks(\n",
    "                low_res_masks,\n",
    "                input_size=resized_hw,\n",
    "                original_size=original_size,\n",
    "            )\n",
    "\n",
    "            gt_t = preprocess_mask_np(gt).to(DEVICE)\n",
    "\n",
    "            d, j = dice_iou_from_logits(up_masks, gt_t)\n",
    "            dice_list.append(d)\n",
    "            iou_list.append(j)\n",
    "\n",
    "            count += 1\n",
    "            if count >= max_instances:\n",
    "                break\n",
    "        if count >= max_instances:\n",
    "            break\n",
    "\n",
    "    sam_model.train()\n",
    "    return float(np.mean(dice_list)) if dice_list else 0.0, float(np.mean(iou_list)) if iou_list else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nApc4gGCFw8Q",
    "outputId": "b285cd0c-52a0-4db4-c9f7-65fb1c287c91"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEpoch 1/20:   0%|          | 0/1260 [00:00<?, ?it/s]/tmp/ipython-input-4067520914.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=AMP):\n",
      "Epoch 1/20:   3%|▎         | 32/1260 [01:54<1:24:23,  4.12s/it, loss=0.3042]/tmp/ipython-input-4067520914.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=AMP):\n",
      "Epoch 1/20:   3%|▎         | 36/1260 [02:11<1:31:19,  4.48s/it, loss=0.2999]/tmp/ipython-input-4067520914.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=AMP):\n",
      "Epoch 1/20: 100%|██████████| 1260/1260 [1:29:46<00:00,  4.27s/it, loss=0.2487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Train loss: 0.248732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 VAL Dice: 0.9016 | VAL IoU: 0.8248\n",
      "✅ Saved best: /content/drive/MyDrive/Kuliah/Skripsi S1/checkpoints/sam_xbd_maskdecoder_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 1260/1260 [1:29:16<00:00,  4.25s/it, loss=0.2289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Train loss: 0.228868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 VAL Dice: 0.9063 | VAL IoU: 0.8326\n",
      "✅ Saved best: /content/drive/MyDrive/Kuliah/Skripsi S1/checkpoints/sam_xbd_maskdecoder_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 1260/1260 [1:29:54<00:00,  4.28s/it, loss=0.2242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Train loss: 0.224161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 VAL Dice: 0.9022 | VAL IoU: 0.8261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 1260/1260 [1:29:40<00:00,  4.27s/it, loss=0.2202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Train loss: 0.220219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 VAL Dice: 0.9041 | VAL IoU: 0.8296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20:   3%|▎         | 43/1260 [03:14<57:49,  2.85s/it, loss=0.2232]  "
     ]
    }
   ],
   "source": [
    "best_dice = -1.0\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    sam.train()\n",
    "    running_loss = 0.0\n",
    "    seen = 0\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    for imgs, gts, bboxes in pbar:\n",
    "        total_loss_this_batch = 0.0\n",
    "\n",
    "        for i in range(len(imgs)):\n",
    "            img = imgs[i]\n",
    "            gt  = gts[i]\n",
    "            bbox = bboxes[i].cpu().numpy()\n",
    "\n",
    "            original_size = img.shape[:2]\n",
    "\n",
    "            input_image, resized_hw = preprocess_image_np(img)\n",
    "            with torch.no_grad():\n",
    "                image_embedding = sam.image_encoder(input_image)\n",
    "\n",
    "            box = transform.apply_boxes(bbox[None, :].astype(np.float32), original_size)\n",
    "            box_torch = torch.as_tensor(box, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                sparse_embeddings, dense_embeddings = sam.prompt_encoder(\n",
    "                    points=None, boxes=box_torch, masks=None\n",
    "                )\n",
    "\n",
    "            gt_t = preprocess_mask_np(gt).to(DEVICE)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=AMP):\n",
    "                low_res_masks, _ = sam.mask_decoder(\n",
    "                    image_embeddings=image_embedding,\n",
    "                    image_pe=sam.prompt_encoder.get_dense_pe(),\n",
    "                    sparse_prompt_embeddings=sparse_embeddings,\n",
    "                    dense_prompt_embeddings=dense_embeddings,\n",
    "                    multimask_output=False,\n",
    "                )\n",
    "\n",
    "                up_masks = sam.postprocess_masks(\n",
    "                    low_res_masks,\n",
    "                    input_size=resized_hw,\n",
    "                    original_size=original_size,\n",
    "                )\n",
    "\n",
    "                loss = 0.5 * bce(up_masks, gt_t) + 0.5 * dice_loss(up_masks, gt_t)\n",
    "\n",
    "            loss = loss / GRAD_ACCUM_STEPS\n",
    "            scaler.scale(loss).backward()\n",
    "            total_loss_this_batch += loss.item()\n",
    "\n",
    "        if (global_step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        global_step += 1\n",
    "        running_loss += total_loss_this_batch\n",
    "        seen += 1\n",
    "\n",
    "        avg_loss = running_loss / max(seen, 1)\n",
    "        pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\"})\n",
    "\n",
    "    epoch_loss = running_loss / max(seen, 1)\n",
    "    print(f\"\\nEpoch {epoch} Train loss: {epoch_loss:.6f}\")\n",
    "\n",
    "    # Evaluate\n",
    "    val_dice, val_iou = evaluate_sam(sam, val_loader, max_instances=800)\n",
    "    print(f\"Epoch {epoch} VAL Dice: {val_dice:.4f} | VAL IoU: {val_iou:.4f}\")\n",
    "\n",
    "    if val_dice > best_dice:\n",
    "        best_dice = val_dice\n",
    "        save_path = os.path.join(OUT_DIR, \"sam_xbd_maskdecoder_best.pth\")\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_type\": MODEL_TYPE,\n",
    "            \"sam_ckpt\": SAM_CKPT,\n",
    "            \"mask_decoder_state_dict\": sam.mask_decoder.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"best_dice\": best_dice,\n",
    "            \"val_iou\": val_iou,\n",
    "        }, save_path)\n",
    "        print(\"✅ Saved best:\", save_path)\n",
    "\n",
    "print(\"\\nDONE. Best Val Dice:\", best_dice)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMS+InT9gJAWpOj630hr1io",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
